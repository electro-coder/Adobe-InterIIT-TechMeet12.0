@inproceedings{borkar1998stability,
  title={Stability and convergence of stochastic approximation using the ODE method},
  author={Borkar, VS and Meyn, SP},
  booktitle={Proceedings of the 37th IEEE Conference on Decision and Control (Cat. No. 98CH36171)},
  volume={1},
  pages={277--282},
  year={1998},
  organization={IEEE}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}
@article{kushner1997applications,
  title={Applications to learning, state dependent noise, and queueing},
  author={Kushner, Harold J and Yin, G George and Kushner, Harold J and Yin, G George},
  journal={Stochastic Approximation Algorithms and Applications},
  pages={25--46},
  year={1997},
  publisher={Springer}
}
@Inbook{Bhatnagar2013,
author="Bhatnagar, S.
and Prasad, H.
and Prashanth, L.",
title="Kiefer-Wolfowitz Algorithm",
bookTitle="Stochastic Recursive Algorithms for Optimization: Simultaneous Perturbation Methods",
year="2013",
publisher="Springer London",
address="London",
pages="31--39",
abstract="In this chapter, we review the Finite Difference Stochastic Approximation (FDSA) algorithm, also known as Kiefer-Wolfowitz (K-W) algorithm, and some of its variants for finding a local minimum of an objective function. The K-W scheme is a version of the Robbins-Monro stochastic approximation algorithm and incorporates balanced two-sided estimates of the gradient using two objective function measurements for a scalar parameter. When the parameter is an N-dimensional vector, the number of function measurements using this algorithm scales up to 2N. A one-sided variant of this algorithm in the latter case requires N{\thinspace}+{\thinspace}1 function measurements. We present the original K-W scheme, first for the case of a scalar parameter, and subsequently for a vector parameter of arbitrary dimension. Variants including the one-sided version are then presented. We only consider here the case when the objective function is a simple expectation over noisy cost samples and not when it has a long-run average form. The latter form of the cost objective would require multi-timescale stochastic approximation, the general case of which was discussed in Chapter 3. Stochastic algorithms for the long-run average cost objectives will be considered in later chapters.",
isbn="978-1-4471-4285-0",
doi="10.1007/978-1-4471-4285-0_4",
url="https://doi.org/10.1007/978-1-4471-4285-0_4"
}
@article{durand2020zap,
  title={Zap Stochastic Approximation and Reinforcement Learning},
  author={Durand, Fran{\c{c}}ois and Devraj, Adithya M and Meyn, Sean},
  year={2020}
}
